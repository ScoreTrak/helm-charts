helm upgrade --timeout 600s --install release ./charts/scoretrak --dry-run
Release "release" does not exist. Installing it now.
NAME: release
LAST DEPLOYED: Sun Oct 10 19:40:08 2021
NAMESPACE: default
STATUS: pending-install
REVISION: 1
HOOKS:
---
# Source: scoretrak/charts/client/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-client-test-connection"
  labels:
    helm.sh/chart: client-0.2.7
    app.kubernetes.io/name: client
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeded
spec:
  containers:
    - name: curl
      image: curlimages/curl:latest
      command: ['curl']
      args: ['release-client:80']
  restartPolicy: Never
---
# Source: scoretrak/charts/cockroachdb/templates/tests/client.yaml
kind: Pod
apiVersion: v1
metadata:
  name: release-cockroachdb-test
  namespace: "default"
  annotations:
    helm.sh/hook: test-success
spec:
  restartPolicy: Never
  containers:
    - name: client-test
      image: "cockroachdb/cockroach:v21.1.8"
      imagePullPolicy: "IfNotPresent"
      command:
        - /cockroach/cockroach
        - sql
        - --insecure
        - --host
        - release-cockroachdb-public.default
        - --port
        - "26257"
        - -e
        - SHOW DATABASES;
---
# Source: scoretrak/charts/envoy/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-envoy-test-connection"
  labels:
    helm.sh/chart: envoy-0.2.15
    app.kubernetes.io/name: envoy
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v1.19.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['release-envoy:80']
  restartPolicy: Never
---
# Source: scoretrak/charts/server/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "release-server-test-connection"
  labels:
    helm.sh/chart: server-0.2.18
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.7"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: curl
      image: curlimages/curl:latest
      command: ['curl']
      args: ['release-server:']
  restartPolicy: Never
---
# Source: scoretrak/charts/cockroachdb/templates/job.init.yaml
kind: Job
apiVersion: batch/v1
metadata:
  name: release-cockroachdb-init
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: init
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
spec:
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release"
        app.kubernetes.io/component: init
    spec:
      restartPolicy: OnFailure
      terminationGracePeriodSeconds: 0
      serviceAccountName: release-cockroachdb
      initContainers:
        # The init-certs container sends a CSR (certificate signing request) to
        # the Kubernetes cluster.
        # You can see pending requests using:
        #   kubectl get csr
        # CSRs can be approved using:
        #   kubectl certificate approve <csr-name>
        #
        # In addition to the Node certificate and key, the init-certs entrypoint
        # will symlink the cluster CA to the certs directory.
        - name: init-certs
          image: "cockroachdb/cockroach-k8s-request-cert:0.4"
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/ash
            - -ecx
            - >-
              /request-cert
              -namespace=${POD_NAMESPACE}
              -certs-dir=/cockroach-certs/
              -symlink-ca-from=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              -type=client
              -user=root
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: client-certs
              mountPath: /cockroach-certs/
      containers:
        - name: cluster-init
          image: "cockroachdb/cockroach:v21.1.8"
          imagePullPolicy: "IfNotPresent"
          # Run the command in an `while true` loop because this Job is bound
          # to come up before the CockroachDB Pods (due to the time needed to
          # get PersistentVolumes attached to Nodes), and sleeping 5 seconds
          # between attempts is much better than letting the Pod fail when
          # the init command does and waiting out Kubernetes' non-configurable
          # exponential back-off for Pod restarts.
          # Command completes either when cluster initialization succeeds,
          # or when cluster has been initialized already.
          command:
            - /bin/bash
            - -c
            - >-
              while true; do
              initOUT=$(set -x;
              /cockroach/cockroach init
              --certs-dir=/cockroach-certs/
              --host=release-cockroachdb-0.release-cockroachdb:26257
              2>&1);
              initRC="$?";
              echo $initOUT;
              [[ "$initRC" == "0" ]] && exit 0;
              [[ "$initOUT" == *"cluster has already been initialized"* ]] && exit 0;
              sleep 5;
              done
          volumeMounts:
            - name: client-certs
              mountPath: /cockroach-certs/
      volumes:
        - name: client-certs
          emptyDir: {}
MANIFEST:
---
# Source: scoretrak/charts/cockroachdb/templates/poddisruptionbudget.yaml
kind: PodDisruptionBudget
apiVersion: policy/v1beta1
metadata:
  name: release-cockroachdb-budget
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/component: cockroachdb
  maxUnavailable: 1
---
# Source: scoretrak/charts/cockroachdb/templates/serviceaccount.yaml
kind: ServiceAccount
apiVersion: v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
---
# Source: scoretrak/charts/server/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-server
  labels:
    helm.sh/chart: server-0.2.18
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.7"
    app.kubernetes.io/managed-by: Helm
---
# Source: scoretrak/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: releaserelease-db-client-secret
  labels:
  
    helm.sh/chart: scoretrak-0.1.2
    app.kubernetes.io/name: scoretrak
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
  annotations:
    kubernetes.io/service-account.name: release-cockroachdb
type: kubernetes.io/service-account-token
---
# Source: scoretrak/templates/envoy-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-envoy
  labels:
    helm.sh/chart: envoy-%!s(<nil>)
    app.kubernetes.io/name: envoy
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
data:
  envoy.yaml: |
    admin:
      access_log_path: /tmp/admin_access.log
      address:
        socket_address: { address: 127.0.0.1, port_value: 9901 }

    static_resources:
      listeners:
      - name: listener_0
        address:
          socket_address: { address: 0.0.0.0, port_value: 80 }
        filter_chains:
        - filters:
          - name: envoy.filters.network.http_connection_manager
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
              codec_type: auto
              stat_prefix: ingress_http
              route_config:
                name: routes
                virtual_hosts:
                - name: scoretrak_service
                  domains: ["scoretrak.example.com"]
                  routes:
                  - match:
                      prefix: "/"
                      headers:
                      - name: "Content-Type"
                        prefixMatch: "application/grpc-web-text"
                    route:
                      cluster: scoretrak-server
                      timeout: 0s
                      max_stream_duration:
                        grpc_timeout_header_max: 0s
                  - match:
                      prefix: "/"
                    route:
                      cluster: scoretrak-client
                  cors:
                    allow_origin_string_match:
                      - prefix: "*"
                    allow_methods: GET, PUT, DELETE, POST, OPTIONS
                    allow_headers: keep-alive,user-agent,cache-control,content-type,content-transfer-encoding,custom-header-1,x-accept-content-transfer-encoding,x-accept-response-streaming,x-user-agent,x-grpc-web,grpc-timeout,authorization
                    max_age: "1728000"
                    expose_headers: custom-header-1,grpc-status,grpc-message

      clusters:
        - name: scoretrak-server
          connect_timeout: 0.25s
          type: logical_dns
          http2_protocol_options: {}
          lb_policy: round_robin
          load_assignment:
            cluster_name: scoretrak-server
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          address: release-server
                          port_value: 380

        - name: scoretrak-client
          connect_timeout: 0.25s
          type: logical_dns
          lb_policy: round_robin
          load_assignment:
            cluster_name: scoretrak-client
            endpoints:
              - lb_endpoints:
                  - endpoint:
                      address:
                        socket_address:
                          address: release-client
                          port_value: 80
---
# Source: scoretrak/templates/server-configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: release-server
  labels:
    helm.sh/chart: server-%!s(<nil>)
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
data:
  config.yaml: |-
    
    queue:
      use: nsq
      nsq:
        
        producernsqd: "release-nsqd:4150"
        
        nsqlookupd:
        
        - --lookupd-tcp-address=release-nsqlookupd-0.release-nsqlookupd:4160
        - --lookupd-tcp-address=release-nsqlookupd-1.release-nsqlookupd:4160
        - --lookupd-tcp-address=release-nsqlookupd-2.release-nsqlookupd:4160
        

    port: 380
    production: false

    db:
      cockroach:
        host: release-cockroachdb-public
        port: 26257
        database: scoretrak
    
        clientca: /cockroach-certs/ca.crt
        clientsslkey: /cockroach-certs/client.root.key
        clientsslcert: /cockroach-certs/client.root.crt
    

    platform:
      use: kubernetes

    jwt:
      secret: CXVGPwHVLTGKjG5CvpVx2HBx
---
# Source: scoretrak/charts/cockroachdb/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: ["certificates.k8s.io"]
    resources: ["certificatesigningrequests"]
    verbs: ["create", "get", "watch"]
---
# Source: scoretrak/charts/server/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-server
  namespace: default
rules:
  - resources:
      - daemonsets
      - pods
    apiGroups:
      - extensions
      - apps
    verbs:
      - create
      - get
      - list
      - watch
      - delete
      - update
---
# Source: scoretrak/charts/cockroachdb/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-cockroachdb
subjects:
  - kind: ServiceAccount
    name: release-cockroachdb
    namespace: "default"
---
# Source: scoretrak/charts/server/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: release-server
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-server
subjects:
  - kind: ServiceAccount
    name:  release-server
    namespace: default
---
# Source: scoretrak/charts/cockroachdb/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["create", "get"]
---
# Source: scoretrak/charts/cockroachdb/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-cockroachdb
subjects:
  - kind: ServiceAccount
    name: release-cockroachdb
    namespace: "default"
---
# Source: scoretrak/charts/client/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-client
  labels:
    helm.sh/chart: client-0.2.7
    app.kubernetes.io/name: client
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      name: http
  selector:
    app.kubernetes.io/name: client
    app.kubernetes.io/instance: release
---
# Source: scoretrak/charts/cockroachdb/templates/service.discovery.yaml
# This service only exists to create DNS entries for each pod in
# the StatefulSet such that they can resolve each other's IP addresses.
# It does not create a load-balanced ClusterIP and should not be used directly
# by clients in most circumstances.
kind: Service
apiVersion: v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
  annotations:
    # Use this annotation in addition to the actual field below because the
    # annotation will stop being respected soon, but the field is broken in
    # some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running
    # in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: _status/vars
    prometheus.io/port: "8080"
spec:
  clusterIP: None
  # We want all Pods in the StatefulSet to have their addresses published for
  # the sake of the other CockroachDB Pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/component: cockroachdb
---
# Source: scoretrak/charts/cockroachdb/templates/service.public.yaml
# This Service is meant to be used by clients of the database.
# It exposes a ClusterIP that will automatically load balance connections
# to the different database Pods.
kind: Service
apiVersion: v1
metadata:
  name: release-cockroachdb-public
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
  annotations:
    service.alpha.kubernetes.io/app-protocols: '{"http":"HTTPS"}'
spec:
  type: "ClusterIP"
  ports:
    # The main port, served by gRPC, serves Postgres-flavor SQL, inter-node
    # traffic and the CLI.
    - name: "grpc"
      port: 26257
      targetPort: grpc
    # The secondary port serves the UI as well as health and debug endpoints.
    - name: "http"
      port: 8080
      targetPort: http
  selector:
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/component: cockroachdb
---
# Source: scoretrak/charts/envoy/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-envoy
  labels:
    helm.sh/chart: envoy-0.2.15
    app.kubernetes.io/name: envoy
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v1.19.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
    - port: 9901
      targetPort: 9901
      protocol: TCP
      name: admin
  selector:
    app.kubernetes.io/name: envoy
    app.kubernetes.io/instance: release
---
# Source: scoretrak/charts/nsq/templates/nsqadmin-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-nsqadmin
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - port: 4171
    targetPort: http
  selector:
    app.kubernetes.io/component: nsqadmin
---
# Source: scoretrak/charts/nsq/templates/nsqd-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-nsqd
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 4151
    protocol: TCP
    targetPort: http
  - name: tcp
    port: 4150
    protocol: TCP
    targetPort: tcp
  selector:
    app.kubernetes.io/component: nsqd
---
# Source: scoretrak/charts/nsq/templates/nsqlookupd-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-nsqlookupd
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
  - port: 4160
    name: tcp
    targetPort: 4160
  - port: 4161
    name: http
    targetPort: 4161
  selector:
    app.kubernetes.io/component: nsqlookupd
---
# Source: scoretrak/charts/server/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-server
  labels:
    helm.sh/chart: server-0.2.18
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.7"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 380
      targetPort: 380
      name: grpc
  selector:
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
---
# Source: scoretrak/charts/client/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-client
  labels:
    helm.sh/chart: client-0.2.7
    app.kubernetes.io/name: client
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: client
      app.kubernetes.io/instance: release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: client
        app.kubernetes.io/instance: release
    spec:
      containers:
        - name: client
          image: "ghcr.io/scoretrak/client/scoretrak-client:v0.2.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
---
# Source: scoretrak/charts/envoy/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-envoy
  labels:
    helm.sh/chart: envoy-0.2.15
    app.kubernetes.io/name: envoy
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v1.19.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: envoy
      app.kubernetes.io/instance: release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: envoy
        app.kubernetes.io/instance: release
    spec:
      serviceAccountName: default
      securityContext:
        {}
      containers:
        - name: envoy
          securityContext:
            {}
          image: "envoyproxy/envoy:v1.19.1"
          imagePullPolicy: IfNotPresent
          command:
            - envoy
          args:
            - -c
            - /config/envoy.yaml
          volumeMounts:
            - mountPath: /config
              name: config
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: admin
              containerPort: 9901
              protocol: TCP








          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: release-envoy
            optional: false
---
# Source: scoretrak/charts/nsq/templates/nsqadmin-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-nsqadmin
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: nsqadmin
  priorityClassName: 
  template:
    metadata:
      labels:
        app.kubernetes.io/component: nsqadmin
    spec:
      securityContext:
        {}
      containers:
      - name: release-nsqadmin
        image: "nsqio/nsq:v1.2.1"
        imagePullPolicy: IfNotPresent
        command:
        - /nsqadmin
        args:
        - --lookupd-http-address=release-nsqlookupd-0.release-nsqlookupd:4161
        - --lookupd-http-address=release-nsqlookupd-1.release-nsqlookupd:4161
        - --lookupd-http-address=release-nsqlookupd-2.release-nsqlookupd:4161
        
        terminationMessagePolicy: FallbackToLogsOnError
        ports:
        - containerPort: 4171
          name: http
        livenessProbe:
          httpGet:
            path: /
            port: http
        readinessProbe:
          httpGet:
            path: /
            port: http
        resources:
            null
      serviceAccountName:
---
# Source: scoretrak/charts/server/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-server
  labels:
    helm.sh/chart: server-0.2.18
    app.kubernetes.io/name: server
    app.kubernetes.io/instance: release
    app.kubernetes.io/version: "v0.2.7"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: server
      app.kubernetes.io/instance: release
  template:
    metadata:
      labels:
        app.kubernetes.io/name: server
        app.kubernetes.io/instance: release
    spec:
      serviceAccountName: release-server
      initContainers:
        
        - name: init-certs
          image: cockroachdb/cockroach-k8s-request-cert:0.4
          imagePullPolicy: IfNotPresent
          command:
          - "/bin/ash"
          - "-ecx"
          - "/request-cert -namespace=${POD_NAMESPACE} -certs-dir=/cockroach-certs -type=client -user=root -symlink-ca-from=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
          env:
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          volumeMounts:
          - name: client-certs
            mountPath: /cockroach-certs
          - name: cockroachdb-service-account
            mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        
      containers:
        - name: server
          image: "ghcr.io/scoretrak/scoretrak/scoretrak-server:v0.2.7"
          imagePullPolicy: IfNotPresent
          command:
          - ./master
          - -config
          - /scoretrak/config.yaml
          ports:
            - name: grpc
              containerPort: 380
          volumeMounts:
            - name: scoretrak-config
              mountPath: /scoretrak
            
            - name: client-certs
              mountPath: /cockroach-certs
            
          resources:
            {}
      volumes:
        - name: scoretrak-config
          configMap:
            name: release-server
        
        - name: client-certs
          emptyDir: {}
        - name: cockroachdb-service-account
          secret:
            
            secretName: releaserelease-db-client-secret
---
# Source: scoretrak/charts/cockroachdb/templates/statefulset.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: release-cockroachdb
  namespace: "default"
  labels:
    helm.sh/chart: cockroachdb-6.0.9
    app.kubernetes.io/name: cockroachdb
    app.kubernetes.io/instance: "release"
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/component: cockroachdb
spec:
  serviceName: release-cockroachdb
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app.kubernetes.io/name: cockroachdb
      app.kubernetes.io/instance: "release"
      app.kubernetes.io/component: cockroachdb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cockroachdb
        app.kubernetes.io/instance: "release"
        app.kubernetes.io/component: cockroachdb
    spec:
      serviceAccountName: release-cockroachdb
      initContainers:
        # The init-certs container sends a CSR (certificate signing request) to
        # the Kubernetes cluster.
        # You can see pending requests using:
        #   kubectl get csr
        # CSRs can be approved using:
        #   kubectl certificate approve <csr-name>
        #
        # All addresses used to contact a Node must be specified in the
        # `--addresses` arg.
        #
        # In addition to the Node certificate and key, the init-certs entrypoint
        # will symlink the cluster CA to the certs directory.
        - name: init-certs
          image: "cockroachdb/cockroach-k8s-request-cert:0.4"
          imagePullPolicy: "IfNotPresent"
          command:
            - /bin/ash
            - -ecx
            - >-
              /request-cert
              -namespace=${POD_NAMESPACE}
              -certs-dir=/cockroach-certs/
              -symlink-ca-from=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              -type=node
              -addresses=localhost,127.0.0.1,$(hostname -f),$(hostname -f|cut -f 1-2 -d '.'),release-cockroachdb-public,release-cockroachdb-public.$(hostname -f|cut -f 3- -d '.')
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: certs
              mountPath: /cockroach-certs/
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                topologyKey: kubernetes.io/hostname
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: cockroachdb
                    app.kubernetes.io/instance: "release"
                    app.kubernetes.io/component: cockroachdb
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: cockroachdb
            app.kubernetes.io/instance: "release"
            app.kubernetes.io/component: cockroachdb
        maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      containers:
        - name: db
          image: "cockroachdb/cockroach:v21.1.8"
          imagePullPolicy: "IfNotPresent"
          args:
            - shell
            - -ecx
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            #
            # `--join` CLI flag is hardcoded to exactly 3 Pods, because:
            # 1. Having `--join` value depending on `statefulset.replicas`
            #    will trigger undesired restart of existing Pods when
            #    StatefulSet is scaled up/down. We want to scale without
            #    restarting existing Pods.
            # 2. At least one Pod in `--join` is enough to successfully
            #    join CockroachDB cluster and gossip with all other existing
            #    Pods, even if there are 3 or more Pods.
            # 3. It's harmless for `--join` to have 3 Pods even for 1-Pod
            #    clusters, while it gives us opportunity to scale up even if
            #    some Pods of existing cluster are down (for whatever reason).
            # See details explained here:
            # https://github.com/helm/charts/pull/18993#issuecomment-558795102
            - >-
              exec /cockroach/cockroach
              start --join=${STATEFULSET_NAME}-0.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-1.${STATEFULSET_FQDN}:26257,${STATEFULSET_NAME}-2.${STATEFULSET_FQDN}:26257
              --advertise-host=$(hostname).${STATEFULSET_FQDN}
              --logtostderr=INFO
              --certs-dir=/cockroach/cockroach-certs/
              --http-port=8080
              --port=26257
              --cache=500Mi
              --max-offset=1000ms
              --max-sql-memory=500Mi
          env:
            - name: STATEFULSET_NAME
              value: release-cockroachdb
            - name: STATEFULSET_FQDN
              value: release-cockroachdb.default.svc.cluster.local
            - name: COCKROACH_CHANNEL
              value: kubernetes-helm
          ports:
            - name: grpc
              containerPort: 26257
              protocol: TCP
            - name: http
              containerPort: 8080
              protocol: TCP
          volumeMounts:
            - name: datadir
              mountPath: /cockroach/cockroach-data/
            - name: certs
              mountPath: /cockroach/cockroach-certs/
          livenessProbe:
            httpGet:
              path: /health
              port: http
              scheme: HTTPS
            initialDelaySeconds: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /health?ready=1
              port: http
              scheme: HTTPS
            initialDelaySeconds: 10
            periodSeconds: 5
            failureThreshold: 2
          resources:
            limits:
              memory: 2Gi
            requests:
              memory: 2Gi
      volumes:
        - name: datadir
          emptyDir: {}
        - name: certs
          emptyDir: {}
---
# Source: scoretrak/charts/nsq/templates/nsqd-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-nsqd
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  podManagementPolicy: Parallel
  serviceName: "release-nsqd-headless"
  selector:
    matchLabels:
      app.kubernetes.io/component: nsqd
  priorityClassName: 
  template:
    metadata:
      labels:
        app.kubernetes.io/component: nsqd
    spec:
      securityContext:
        {}
      containers:
      - name: release-nsqd
        image: "nsqio/nsq:v1.2.1"
        imagePullPolicy: IfNotPresent
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        command:
        - /nsqd
        args:
        - --broadcast-address=$(POD_IP)
        - --lookupd-tcp-address=release-nsqlookupd-0.release-nsqlookupd:4160
        - --lookupd-tcp-address=release-nsqlookupd-1.release-nsqlookupd:4160
        - --lookupd-tcp-address=release-nsqlookupd-2.release-nsqlookupd:4160
        - -data-path=/data
        terminationMessagePolicy: FallbackToLogsOnError
        ports:
        - containerPort: 4150
          name: tcp
        - containerPort: 4151
          name: http
        readinessProbe:
          httpGet:
            path: /ping
            port: http
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /ping
            port: http
          initialDelaySeconds: 10
          timeoutSeconds: 5
        resources:
            {}
        volumeMounts:
        - name: data
          mountPath: /data
      serviceAccountName: 
      terminationGracePeriodSeconds: 5
      volumes:
      - name: "data"
        emptyDir: {}
---
# Source: scoretrak/charts/nsq/templates/nsqlookupd-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-nsqlookupd
  labels:
    helm.sh/chart: nsq-1.2.0
    helm.sh/release: release
    app.kubernetes.io/name: nsq
    app.kubernetes.io/instance: release
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  podManagementPolicy: Parallel
  serviceName: release-nsqlookupd
  selector:
    matchLabels:
      app.kubernetes.io/component: nsqlookupd
  priorityClassName: 
  template:
    metadata:
      labels:
        app.kubernetes.io/component: nsqlookupd
    spec:
      securityContext:
        {}
      containers:
      - name: release-nsqlookupd
        image: "nsqio/nsq:v1.2.1"
        imagePullPolicy: IfNotPresent
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        command:
        - /nsqlookupd
        args:
        - --broadcast-address=$(POD_IP)
        terminationMessagePolicy: FallbackToLogsOnError
        ports:
        - containerPort: 4160
          name: tcp
        - containerPort: 4161
          name: http
        readinessProbe:
          httpGet:
            path: /ping
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          httpGet:
            path: /ping
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          timeoutSeconds: 5
        resources:
            {}
      serviceAccountName:

